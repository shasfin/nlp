1 	16/02/2015 	Introduction & Project overview (EA,MC)

    Challenges are:
    - Ambiguity (at all levels)
	- Sparsity
	- World knowledge
	Levels:
	- Words (tokenization, in English there are whitespaces, in Chinese it's even more difficult)
	- Syntax
	- Semantics
	- Discourse

	
2 	23/02/2015 	Entity Disambiguation (MC)  -- NOT IN THE EXAM

    An entity is one "thing" uniquely identifiable in a knowledge base
	Challenges:
	- Ambiguity (same word means different things)
	- Variability (different words mean the same thing)
	Named Entity Recognition (recognize and tag named entities such as 'Bank of America' - organization)
	Word Sense Disambiguation (which meaning of the word is used in a particular sentence)
		most used corpus: WordNet
	Approaches:
	- Wikipedia-based entity linking (that with Lara Croft)
	- TagME
	- Optimization (with integer linear programming)
	- Structured learning (joint mention detection, svm)
	- Graph based (solution belongs to dense subgraph or has higher page rank)
	- Topic modeling
	- Piggyback
	- LDA (Latent Dirichlet allocation, represent a document as a mixture of a small number of topics and each word is attributable to one of the document's topics)
		The topics distribution is assumed to have a Dirichlet prior
	Similarity functions:
	- Link based (Google similarity)
	- Count based (pmi(x,y) = log(p(x,y) / p(x)p(y)))
	Eval metrics:
	- Precision = tp / (tp+fp)  | in the book: (#correct chunks given by the system) / (#chunks given by the system)
	- Recall = tp / (tp + fn)   | in the book: (#correct chunks given by the system) / (#actual chunks in the text)
	- F1 = 2*Precision*Recall / (Precision + Recall) = harmonic mean of precision and recall
	

3 	02/03/2015 	Finite State Automata and Morphology (EA)

	Mathematiker, Logiker, Informatiker:
	-Gottlob Frege: formale Sprache
	-David Hilbert: versucht widerspruchsfreie Axiomatisierung der Mathematik
	-Bertrand Russel: Typentheorie, Entscheidungsproblem
	-Alfred Whitehead: Logik (mit B.Russel)
	-Kurt Gödel: Prädikatenlogik, Unvollständigkeitssätze
	-Alan Turing: Turingmaschine, Turing-Test
	-Claude Shannon: Informationstheorie
	-Noam Chomsky: Grammatik, Sprachenhierarchie
	Chomsky-Hierarchie:
	- type 0 | unrestricted | Turing machine
	- type 1 | context sensitive | Linear bounded automaton
	- type 2 | context free | pushdown automaton
	- type 3 | regular | finite automaton
	Regular languages (definition)
	- the empty language is regular
	- the language containing only the empty word is regular
	- the language containing only one symbol of the alphabet is regular
	- if L1 and L2 are regular languages, then also their concatenation, their union and their Kleene closure are regular languages
	Regular expressions (Perl syntax)
	- /word/   -> matches "word"
	- /[word]/ -> disjunction, matches "w" "o" "r" or "d"
	- /[0-5]/  -> disjunction with a range, matches "0", "1", "2", ... or "5"
	- /[Ww]ord/ -> matches "Word" or "word"
	- /[^wm]ord/ -> matches "Word", "lord", "aord"..., any word with 4 symbols where the first one is NOT "w" and NOT "m" and the last three are "ord"
	- /[w^]ord/ -> matches "word" or "^ord" (caret has a special meaning only at the beginning of a square brace)
	- /^[Ww]ord$/ -> matches only a stream containing "Word" or "word" without other symbold before or after (e.g. a line containing only "Word")
	- /\d/ -> matches any digit
	- /\D/ -> matches any non-digit
	- /\w/ -> matches any alphanumerit symbol or underscore
	- /\W/ -> matches any non-alphanumeric
	- /\s/ -> matches a whitespace (tab, newline,...)
	- /\S/ -> matches non-whitespace
	- /w*ord/ -> matches "ord" or "word" or "wword" or "wwword" or ...
	- /w+ord/ -> matches "word" or "wword" or "wwword" or ...
	- /w?ord/ -> matches "ord" or "word" and nothing else
	- /.word/ -> matches "Lord" or "!ord" or "7ord" or... any word with 4 symbols where the last three are "ord"
	- /(word)|(book)/ -> matches either "word" or "book"
	- /word|book/ -> matches either "wordook" or "worbook"
	- escape special symbols (*,|,.,+,?,...) with a backslash if you want to use them as normal symbols
	- /word?/ -> matches "wor" or "word"
	- /word\?/ -> matches "word?"
	Finite-State Automata
	Morphology
	- morphemes are divided into two classes: stems and affixes
	- inflectional morphology: combine stems and affixes so that the resulting word has the same word class and is transparently related to the original word, although gramatically/semantically different (e.g. walk-walks-walked, word-words)
	- derivational morphology: combine stems and affixes so that the resulting word has (usually but not always) another word class and a not so easy predictable meaning (e.g. computerize-computerization, kll-killer, clear-uncler, happy-happier-happiest)
	English inflectional morphology
	- nouns: markers for plural and possessive
	- verbs: present simple 3rd singular, perfect, preterite and progressive/gerund
	- there are regulars, regulars with somewhat more complex rules and irregulars. New words are almost always regular
	English derivational morphology
	- very complicated and difficult to capture exactly, has a quasi-systematic structure and impredictable meaning and contraints (e.g. unclear is ok, unbig is not English)
	Finite State Automata for Morphology:
	- big, complicated and error-prone, capture well the regular part of the inflectional morphology
	Finite State Transducers for Morphology
	- more powerful than finite-state automata
	- can be seen as recognizer of a relation, as generator (generates a sequence of pairs), as translator (translate one sequence into another), as set relator (computer a relation)
	- we can apply more FST one after the other, translating a lexical form into an intermediate form into a surface form (or back for morphological analysis)
	- in FST the path to an accept state does matter, since it represents a different parse (cfr un-ion-ize-able and union-ize-able)
		how to deal with ambiguity:
		* simply take the first ouput
		* find all the possible outputs and return them all
		* bias the search so that only one or a few likely paths are explored
	- we can compose two FST into one FST, but the number of state will increase a lot
		* create a set of new states that correspond to each pair of sttes from the original machines
		* create a new transition function d3 from d2 and d1 so that d3((xa,ya),i:o) = (xb,yb) iff there is a c such that d1(xa,i:c) = xb and d2(ya,c:o) = yb where i,o,c are tape symbols and xa,ya,xb,yb are states
	Flex
	- we can build a flex scanner to recognize words from a dictionary dump (achtung performance, do not use too many words)
	- or we can lemmatize the words writing some hand-written rules (cfr. Porter Stemmer)
	Tokenization
	- divide text up into tokens. In English you could do it using space and punctuation as word boundary (and have problems with Mr. Dr. 23.4 and similar), in Chinese it is even harder (maximum matching is a good baseline)
	- in English usually solved with simple regular expressions to deal with possessive markers and clitics (I've, don't, you'll), numbers, abbreviations, quotations and punctuation
	- combined with Named Entity Recognizers
	Sentence splitting
	- some markers (?,!) are unambiguous, other (.,,) are ambiguous.
	Spelling correction
	- non-word error: the error results in a non-existing word (e.g. graffe for giraffe)
	- real-word error: the error results in a real word (e.g. "Do you want apples OF pineapples" for "Do you want apples OR pineapples")
	- isolated-word error correction: correct the word looking only at the word itself (without looking at the context)
		operations: insertion, deletion, substitution
		find closest dictionary word in terms of edit distance
	- context-dependent error detection and correction: look at the context (the only way to detect and correct real-word errors)
	

4 	09/03/2015 	POS tagging and HMMs (MC)

	Word classes
	- open word class: generally big, new words can be invented.
		noun, verb, adjective, adverb
		not all languages have all four of them (but many of them do)
	- closed word class: small stable set of function words
		prepositions, determiners, pronouns, conjunctions, auxiliary verbs, particles, numerals
	TODO




5 	16/03/2015 	Language Models (EA) 
6 	23/03/2015 	Machine Translation (MC) 	
7 	30/03/2015 	Lexical Semantics (EA)

	Word similarity measures:
	- thesaurus based
		* path-length similarity (some function of the #edges between two words in the graph defined by the thesaurus)
			word similarity is usually defined as the maximum of the word sense similarity
			assumption: each link in the network represents a uniform distance
		* information-content word-similarity (add probabilistic information derived from the corpus to the structure of the thesaurus)
			Resnik similarity: information content of the lowest common subsumer
			Lin similarity: ratio between commonality (twice the information content of the lowest subsumer) and full description of the words (sum of their information contents)
			Jiang-Conrath distance: difference between commonality and full description
	- dictionary based (d.h. no structure information used, only the glosses)
			Extended Lesk measure (or extended gloss overlap)
	- distributional
		Depends on the context
		For each word, collect contexts where the word appears, represent each word as a vector of frequencies where each dimension represents a context
		Apart from contextual windows, we can also use other information (bag-of-words, parsing information, POS tags etc.)
		As always, raw frequencies are not the best measure (there are more frequent contexts)
		* assoc(w,f) measures the association between the word w and the (context) feature f
		* sim(v,w) measures the similarity between the vectors representing two words v and w
	- graph based
		* Agirre: calculate a personalized page-rank for each word, compare words by comparing their distributions
	- knowledge based
		Roget's thesaurus (no description), Wikipedia (no description)
	Word similarity and word relatedness
	- two words are similar if they are nearly synonyms
	- two words are related if they are somehow related to each other (antonyms, meronyms, holonyms, ... or are usually used in the same context)

	
8 	20/04/2015 	Text Summarization (EA) 
9 	27/04/2015 	Event Understanding (EA) 
10 	04/05/2015 	Grammars and Parsing (MC)

	Syntax
	- Key concepts
		Constituency
		* a constituent is some group of words acting as a unit (phrase)
		* external evidence:
			constituents sometimes can be moved around in the sentence, but the words forming them must stay together
			constituents can do something that not all word forming the constituent can (e.g. precede verbs)
		* internal evidence:
			the structure of the constituent, e.g. Det + Noun
		Head
		* most "important" word of a constituent
		Subcategorization and agreement
		Grammatical relations and Dependency
	- Key formalism
		Context-free grammars (CFG)
		* they have terminals (actual English words), non-terminals (some symbols like NP, corresponding to constituents) and productions (rules that say how a particular non-terminal can be expanded into something else). There is also a special non-terminal symbol: the start symbol S.
		* lexicon rules are those where the rhs does not contain non-terminals
		* can be automatically rewritten into Chomsky-Normal Form (CNF)
			CNF allows only two types of rules: A -> B C and A -> w
		* CFGs can be used for
			generating strings in the language
			rejecting strings not in the language
			associating syntactic trees to strings in the language
		* problems and limitations
			don't capture agreement and subcategorization constraints (it is possible to generate grammars that do not overgenerate, but they would be huge)
		* summary:
			can be used to model various facts abouts the syntax of a language
			used together with parsers in many applications
			the key phenomenon of constituency can be easily captured with CFGs
			agreement and subcategorization pose significant problems (splitting the rules does not scale)
	- English grammar fragment
		sentence types: declarative, imperative, questions (yes/no questions, wh-questions)
		noun phrase: a head and some modifiers tied together according to some ordering constraints
		* pre-modifiers: articles, numerals, quantifiers, possessives, adjectives and nouns
		* post-modifiers: prepositional phrases, non-finite clauses (e.g. "arriving before noon"), relative clauses (e.g. "hat serve breakfast"), appositives
		agreement: constraints holding among various constituents (e.g. number agreement in NPs, person agreement in VPs)
		verb phrase: a head verb and 0 or more argument constituents
		* subcategorization: subcategorize verbs according to their argument pattern (e.g. transitive/intransitive and so on)
	- Parsing
		assign a tree to an input string s.t. all and only the elements of the input are covered and has an S at the top
		problem: select the correct tree among all the possible trees (deal with ambiguity)
		top-down: start with S, apply grammar rules trying to derive the input string
		* only searches for valid sentences (with an S on top and derived following the rules)
		* but explores also trees that are not consistent with any of the input words
		bottom-up: start with subtrees covering only part of the input (at the beginning a subtree for each word), combine subtrees into larger trees following the rules "backward"
		* only forms trees consistent with the input words
		* but explores structure which won't lead to valid sentences
		CKY: easy bottom-up dynamic programming algorithm for parsing, requires the CFGs to be in CNF
		* build a table so that a non-terminal A spanning from i to j in the input is placed in cell [i,j]
		* if there is an A spanning i,j in the input and A->BC is a rule in the grammar then there must be a B in [i,k] and a C in [k,j] for i<k<j
		* if you have backpointers, you can retrieve a tree from the table. Allow multiple versions of the same non-terminal (ambiguity).
		* since the grammar is in CNF, it is a different tree (not the parse tree we are looking for), but it can be automatically transformed into the tree we are looking for
		* the resulting tree is ambiguous (we can actually read out more than one tree from the table) --> maybe model it with probabilities?
		Earley: another algorithm for parsing, a top-down one
	

 	
11 	11/05/2015 	Statistical and Dependency Parsing (MC)	-- DEPENDENCY PARSING NOT IN THE EXAM