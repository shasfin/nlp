1 	16/02/2015 	Introduction & Project overview (EA,MC)

    Challenges are:
    - Ambiguity (at all levels)
	- Sparsity
	- World knowledge
	Levels:
	- Words (tokenization, in English there are whitespaces, in Chinese it's even more difficult)
	- Syntax
	- Semantics
	- Discourse

	
2 	23/02/2015 	Entity Disambiguation (MC)  -- NOT IN THE EXAM

    An entity is one "thing" uniquely identifiable in a knowledge base
	Challenges:
	- Ambiguity (same word means different things)
	- Variability (different words mean the same thing)
	Named Entity Recognition (recognize and tag named entities such as 'Bank of America' - organization)
	Word Sense Disambiguation (which meaning of the word is used in a particular sentence)
		most used corpus: WordNet
	Approaches:
	- Wikipedia-based entity linking (that with Lara Croft)
	- TagME
	- Optimization (with integer linear programming)
	- Structured learning (joint mention detection, svm)
	- Graph based (solution belongs to dense subgraph or has higher page rank)
	- Topic modeling
	- Piggyback
	- LDA (Latent Dirichlet allocation, represent a document as a mixture of a small number of topics and each word is attributable to one of the document's topics)
		The topics distribution is assumed to have a Dirichlet prior
	Similarity functions:
	- Link based (Google similarity)
	- Count based (pmi(x,y) = log(p(x,y) / p(x)p(y)))
	Eval metrics:
	- Precision = tp / (tp+fp)  | in the book: (#correct chunks given by the system) / (#chunks given by the system)
	- Recall = tp / (tp + fn)   | in the book: (#correct chunks given by the system) / (#actual chunks in the text)
	- F1 = 2*Precision*Recall / (Precision + Recall) = harmonic mean of precision and recall
	

3 	02/03/2015 	Finite State Automata and Morphology (EA)

	Mathematiker, Logiker, Informatiker:
	-Gottlob Frege: formale Sprache
	-David Hilbert: versucht widerspruchsfreie Axiomatisierung der Mathematik
	-Bertrand Russel: Typentheorie, Entscheidungsproblem
	-Alfred Whitehead: Logik (mit B.Russel)
	-Kurt Gödel: Prädikatenlogik, Unvollständigkeitssätze
	-Alan Turing: Turingmaschine, Turing-Test
	-Claude Shannon: Informationstheorie
	-Noam Chomsky: Grammatik, Sprachenhierarchie
	Chomsky-Hierarchie:
	- type 0 | unrestricted | Turing machine
	- type 1 | context sensitive | Linear bounded automaton
	- type 2 | context free | pushdown automaton
	- type 3 | regular | finite automaton
	Regular languages (definition)
	- the empty language is regular
	- the language containing only the empty word is regular
	- the language containing only one symbol of the alphabet is regular
	- if L1 and L2 are regular languages, then also their concatenation, their union and their Kleene closure are regular languages
	Regular expressions (Perl syntax)
	- /word/   -> matches "word"
	- /[word]/ -> disjunction, matches "w" "o" "r" or "d"
	- /[0-5]/  -> disjunction with a range, matches "0", "1", "2", ... or "5"
	- /[Ww]ord/ -> matches "Word" or "word"
	- /[^wm]ord/ -> matches "Word", "lord", "aord"..., any word with 4 symbols where the first one is NOT "w" and NOT "m" and the last three are "ord"
	- /[w^]ord/ -> matches "word" or "^ord" (caret has a special meaning only at the beginning of a square brace)
	- /^[Ww]ord$/ -> matches only a stream containing "Word" or "word" without other symbold before or after (e.g. a line containing only "Word")
	- /\d/ -> matches any digit
	- /\D/ -> matches any non-digit
	- /\w/ -> matches any alphanumerit symbol or underscore
	- /\W/ -> matches any non-alphanumeric
	- /\s/ -> matches a whitespace (tab, newline,...)
	- /\S/ -> matches non-whitespace
	- /w*ord/ -> matches "ord" or "word" or "wword" or "wwword" or ...
	- /w+ord/ -> matches "word" or "wword" or "wwword" or ...
	- /w?ord/ -> matches "ord" or "word" and nothing else
	- /.word/ -> matches "Lord" or "!ord" or "7ord" or... any word with 4 symbols where the last three are "ord"
	- /(word)|(book)/ -> matches either "word" or "book"
	- /word|book/ -> matches either "wordook" or "worbook"
	- escape special symbols (*,|,.,+,?,...) with a backslash if you want to use them as normal symbols
	- /word?/ -> matches "wor" or "word"
	- /word\?/ -> matches "word?"
	Finite-State Automata
	Morphology
	- morphemes are divided into two classes: stems and affixes
	- inflectional morphology: combine stems and affixes so that the resulting word has the same word class and is transparently related to the original word, although gramatically/semantically different (e.g. walk-walks-walked, word-words)
	- derivational morphology: combine stems and affixes so that the resulting word has (usually but not always) another word class and a not so easy predictable meaning (e.g. computerize-computerization, kll-killer, clear-uncler, happy-happier-happiest)
	English inflectional morphology
	- nouns: markers for plural and possessive
	- verbs: present simple 3rd singular, perfect, preterite and progressive/gerund
	- there are regulars, regulars with somewhat more complex rules and irregulars. New words are almost always regular
	English derivational morphology
	- very complicated and difficult to capture exactly, has a quasi-systematic structure and impredictable meaning and contraints (e.g. unclear is ok, unbig is not English)
	Finite State Automata for Morphology:
	- big, complicated and error-prone, capture well the regular part of the inflectional morphology
	Finite State Transducers for Morphology
	- more powerful than finite-state automata
	- can be seen as recognizer of a relation, as generator (generates a sequence of pairs), as translator (translate one sequence into another), as set relator (computer a relation)
	- we can apply more FST one after the other, translating a lexical form into an intermediate form into a surface form (or back for morphological analysis)
	- in FST the path to an accept state does matter, since it represents a different parse (cfr un-ion-ize-able and union-ize-able)
		how to deal with ambiguity:
		* simply take the first ouput
		* find all the possible outputs and return them all
		* bias the search so that only one or a few likely paths are explored
	- we can compose two FST into one FST, but the number of state will increase a lot
		* create a set of new states that correspond to each pair of sttes from the original machines
		* create a new transition function d3 from d2 and d1 so that d3((xa,ya),i:o) = (xb,yb) iff there is a c such that d1(xa,i:c) = xb and d2(ya,c:o) = yb where i,o,c are tape symbols and xa,ya,xb,yb are states
	Flex
	- we can build a flex scanner to recognize words from a dictionary dump (achtung performance, do not use too many words)
	- or we can lemmatize the words writing some hand-written rules (cfr. Porter Stemmer)
	Tokenization
	- divide text up into tokens. In English you could do it using space and punctuation as word boundary (and have problems with Mr. Dr. 23.4 and similar), in Chinese it is even harder (maximum matching is a good baseline)
	- in English usually solved with simple regular expressions to deal with possessive markers and clitics (I've, don't, you'll), numbers, abbreviations, quotations and punctuation
	- combined with Named Entity Recognizers
	Sentence splitting
	- some markers (?,!) are unambiguous, other (.,,) are ambiguous.
	Spelling correction
	- non-word error: the error results in a non-existing word (e.g. graffe for giraffe)
	- real-word error: the error results in a real word (e.g. "Do you want apples OF pineapples" for "Do you want apples OR pineapples")
	- isolated-word error correction: correct the word looking only at the word itself (without looking at the context)
		operations: insertion, deletion, substitution
		find closest dictionary word in terms of edit distance
	- context-dependent error detection and correction: look at the context (the only way to detect and correct real-word errors)
	

4 	09/03/2015 	POS-tagging and HMMs (MC)

	Word classes
	- open word class: generally big, new words can be invented.
		noun, verb, adjective, adverb
		not all languages have all four of them (but many of them do)
	- closed word class: small stable set of function words
		prepositions, determiners, pronouns, conjunctions, auxiliary verbs, particles, numerals
	Part-of-Speech-tagging (POS-tagging)
	- POS tagset (coarse, Penn Treebank, ...)
	- Rule-based tagging (start with a dictionary, assign all possible tags, introduce rules to reduce ambiguity)
	- Statistical (or Stochastic?) models for POS-tagging
		Supervised: annotated training set
		Semisupervised: a part of the training set is annotated
		Unsupervised: only raw text available
	HMM (can be used for supervised POS-tagging) (kind of FSAs with emission probabilities and where the states are "hidden")
	- they have states, observations, transitions with transition probabilities (A) and emissions with emission probabilities (B)
	- Markov assumptions: limited horizon (state depends only on previous state), time-invariant, observation depends only on the state
	- Three tasks:
		1. given an observation, find the most likely sequence of states (decoding) -> Viterbi (max)
		2. given an observation, compute its probability (observation probability) -> forward or backward (sum)
		3. given a dataset of observations, estimate the model's parameters (parameter estimation) -> Baum-Welch (aka forward-backward)
	Bayesian HMM (additional parameter to control sparsity of A and B)
	Information Extraction
	-Segmentation and Labeling (BIO and your favourite classifier, features from the context and also from other sources like gazetteers)
		NER
		Temporal expressions
		*more regular patterns
		*challenging task: normalization (map to interval or point on the timeline)
		Partial parsing (chunking)


5 	16/03/2015 	Language Models (EA)

	Language model: something that assesses P(wn | w1,w2,...,w{n-1})
	N-gram models: approximate this probability looking only at the previous N-1 words
		apply definition of conditional probability and count
		apply chain rule
		problem with easy count: 0 and 0/0 (sparsity)
		Evaluation
		- intrinsic evaluation
			for example perplexity (the lower the better): P(w1,w2,...,wN)^{-1/N}
			some approximation measure, just because extrinsic evaluation is so expensive and time-consuming
		- extrinsic evaluation
			embed into some end-to-end application, see if it does better with the new model
	Smoothing (address the sparsity and the zero counts)
	- Laplace Smoothing (add-one)
		real counts: c_i
		mle probability estimated from the real counts: c_i/N
		laplace counts: c*_i = (c_i+1) N/(N+V) // V is the vocabulary size
		laplace probability estimated from the real counts: (c_i+1)/(N+V)
		as always, we give some probability to the low-probability events by taking some probability away from the high-probability events
		laplace bigram probabilities: (c(w{n-1},wn)+1) / (c(w{n-1}+V)
		not used in practice for N-gram counts
	- Good Turing
		intuition: use the count of things we've seen once to help estimate the count of things we've never seen
		notation: N_x := number of fish species seen x times
		c*_0 = c_1 (use number of species we've seen once to estimate total number of unseen species)
		missing mass (probability mass for all of the unseen events) N_1/N
		c* = (c+1) N_{c+1}/N_c (adjust down all other estimates to get some probability)
		in practice often assume c>k are reliable
	Backoff and Interpolation (use lowerN-grams)
	- Backoff: use only highestN-grams, when available (Katz, don't forget to discount the probabilities, they should still sum up to one)
	- Interpolation: use all the counts, interpolate among them (e.g. linearly) (use held-out development set to set the parameters)


6 	23/03/2015 	Machine Translation (MC)

	Direct
	- word-per-word, maybe with local reordering rules
	Syntactic transfer
	- use syntactic parse trees and rules to convert the trees
	Interlingua
	- go to some universal representation and generate a foreign sentence from the representation
	Compromise between faithfulness and fluency
	Statistical Machine Translation (SMT)
	- best E = argmax_E P(F|E) P(E) // P(F|E) is the translation model (faithfulness), P(E) is the language model (fluency)
	- phrase-based
	- word alignment, spurious words
	Noisy channel model
	IBM Model 1
	- Generative story
		1. Choose length J of the foreign sentence
		2. Choose alignment A (assume all alignments have uniform probability)
		3. for each j choose f_j according to translation table (each foreign word maps to exactly one English word)
	- Probabilistic model
		P(F,A|E) = P(F|E,A)P(A|E)
		P(F|E) = sum_A P(F,A|E)
		each alignment is independent, so we can maximize the dimensions independently
	- EM alignment training
		1. List all alignments, initialize somehow the probabilities t(f_j|e_{a_j})
		2. E-Step: compute the probability of alignments
			P(A|E,F) = P(A,F|E)/P(F|E) = P(A,F|E)/sum_A P(A,F|E)
			P(A,F|E) = prod_j t(f_j|e_{a_j})
		3. M-Step: collect (fractional) counts, normalize
		4. Iterate steps 2 and 3 until convergence
	Symmetrization
	- approximate alignment where the constraint (foreign -> English is injective) does not hold
	- can construct a phrase table from the alignment
	Decoding (A*, best-first, stack decoding)
	Evaluation (human rating or BLEU, NIST, TER, METEOR)


7 	30/03/2015 	Lexical Semantics (EA)

	Word similarity measures:
	- thesaurus based
		* path-length similarity (some function of the #edges between two words in the graph defined by the thesaurus)
			word similarity is usually defined as the maximum of the word sense similarity
			assumption: each link in the network represents a uniform distance
		* information-content word-similarity (add probabilistic information derived from the corpus to the structure of the thesaurus)
			Resnik similarity: information content of the lowest common subsumer
			Lin similarity: ratio between commonality (twice the information content of the lowest subsumer) and full description of the words (sum of their information contents)
			Jiang-Conrath distance: difference between commonality and full description
	- dictionary based (d.h. no structure information used, only the glosses)
			Extended Lesk measure (or extended gloss overlap)
	- distributional
		Depends on the context
		For each word, collect contexts where the word appears, represent each word as a vector of frequencies where each dimension represents a context
		Apart from contextual windows, we can also use other information (bag-of-words, parsing information, POS tags etc.)
		As always, raw frequencies are not the best measure (there are more frequent contexts)
		* assoc(w,f) measures the association between the word w and the (context) feature f
		* sim(v,w) measures the similarity between the vectors representing two words v and w
	- graph based
		* Agirre: calculate a personalized page-rank for each word, compare words by comparing their distributions
	- knowledge based
		Roget's thesaurus (no description), Wikipedia (no description)
	Word similarity and word relatedness
	- two words are similar if they are nearly synonyms
	- two words are related if they are somehow related to each other (antonyms, meronyms, holonyms, ...) or are usually used in the same context)

	
8 	20/04/2015 	Text Summarization (EA)

	-Extractive: synthesize from the sentences from the text
	-Abstractive: use "your own words"
	-Single-document: only one source
	-Multi-document: many sources to the same topic
	-Length (headline, highlights, longer text)
	-Focus (generic/query-focused/opinion-focused)
	-...
	Summarization architecture
	1. Content selection
	2. Information ordering
	3. Sentence resolution/compression
	Summarization based on Rhetorical Parsing
	- Rhetorical Structure Theory (RST): mostly about relations that hold between nucleus and satellite
		evidence, attribution, elaboration, contrast, list, concession, background, condition
	- Parse the input text with a discourse parser, keep the summary of immediate nucleus children
	Unsupervised content selection
	- build a model of important words, select the sentences with most important words
	- common strategies to measure word importance:
		tf-idf (more weight to frequent word in one document that are rare in the collection as a whole)
		log-likelihood ratio
		LDA and topic models
	Supervised content selection
	- binary classification or sentence ranking
	Redundancy removal
	- Maximal Marginal Relevance (MMR): penalize sentences that are similar to already-selected sentences
	Information ordering (chronological, lexical cohesion, centering theory)
	Query-focused summarization (include information from the query)
	Sentence compression (shorten sentences based on their parse tree)
	- search for a dependency subtree under a length constraint in the transformed dependency graph
	- ILP-based compression method: find a subtree rooted in root with the maximum total sum of edge weights w(e) under a character length limit
	- fast compressor (train a max-ent model to decide, at each node, which edges to keep)
	Intrinsic evaluation of text summarization
	- Pyramid: identify the Summary Content Units, count them in manually generated summaries (for computing the weights of each SCU) and in the automatic summary (weight the SCU with the weights)
	- ROUGE (automatic measure, recall-oriented (against gold)


9 	27/04/2015 	Event Understanding (EA)

	Named Entity Recognition (NER) and Classification
	- BIO encoding and your favourite classifier based on contextual features
	Relation detection
	- funny lightly-supervised "Münchhausen" method
	Event Understanding
	- template filling (supervised)
	Open IE
	- self-supervising
	- has a query processor ;)
	- second edition much simpler, uses regular expressions
	- there is a Wikipedia-based Open Extractor
	- web-tables (extract tables from web pages. Alas, most tables are not meaningful to us)


10 	04/05/2015 	Grammars and Parsing (MC)

	Syntax
	- Key concepts
		Constituency
		* a constituent is some group of words acting as a unit (phrase)
		* external evidence:
			constituents sometimes can be moved around in the sentence, but the words forming them must stay together
			constituents can do something that not all word forming the constituent can (e.g. precede verbs)
		* internal evidence:
			the structure of the constituent, e.g. Det + Noun
		Head
		* most "important" word of a constituent
		Subcategorization and agreement
		Grammatical relations and Dependency
	- Key formalism
		Context-free grammars (CFG)
		* they have terminals (actual English words), non-terminals (some symbols like NP, corresponding to constituents) and productions (rules that say how a particular non-terminal can be expanded into something else). There is also a special non-terminal symbol: the start symbol S.
		* lexicon rules are those where the rhs does not contain non-terminals
		* can be automatically rewritten into Chomsky-Normal Form (CNF)
			CNF allows only two types of rules: A -> B C and A -> w
			1. convert unit productions replacing A -> B with A -> beta for each rule B -> beta
			2. convert terminals to dummies replacing A -> beta b gamma with A -> beta X gamma and X -> b
			3. make rules binary
		* CFGs can be used for
			generating strings in the language
			rejecting strings not in the language
			associating syntactic trees to strings in the language
		* problems and limitations
			don't capture agreement and subcategorization constraints (it is possible to generate grammars that do not overgenerate, but they would be huge)
		* summary:
			can be used to model various facts abouts the syntax of a language
			used together with parsers in many applications
			the key phenomenon of constituency can be easily captured with CFGs
			agreement and subcategorization pose significant problems (splitting the rules does not scale)
	- English grammar fragment
		sentence types: declarative, imperative, questions (yes/no questions, wh-questions)
		noun phrase: a head and some modifiers tied together according to some ordering constraints
		* pre-modifiers: articles, numerals, quantifiers, possessives, adjectives and nouns
		* post-modifiers: prepositional phrases, non-finite clauses (e.g. "arriving before noon"), relative clauses (e.g. "hat serve breakfast"), appositives
		agreement: constraints holding among various constituents (e.g. number agreement in NPs, person agreement in VPs)
		verb phrase: a head verb and 0 or more argument constituents
		* subcategorization: subcategorize verbs according to their argument pattern (e.g. transitive/intransitive and so on)
	- Parsing
		assign a tree to an input string s.t. all and only the elements of the input are covered and has an S at the top
		problem: select the correct tree among all the possible trees (deal with ambiguity)
		top-down: start with S, apply grammar rules trying to derive the input string
		* only searches for valid sentences (with an S on top and derived following the rules)
		* but explores also trees that are not consistent with any of the input words
		bottom-up: start with subtrees covering only part of the input (at the beginning a subtree for each word), combine subtrees into larger trees following the rules "backward"
		* only forms trees consistent with the input words
		* but explores structure which won't lead to valid sentences
		CKY: easy bottom-up dynamic programming algorithm for parsing, requires the CFGs to be in CNF
		* build a table so that a non-terminal A spanning from i to j in the input is placed in cell [i,j]
		* if there is an A spanning i,j in the input and A->BC is a rule in the grammar then there must be a B in [i,k] and a C in [k,j] for i<k<j
		* if you have backpointers, you can retrieve a tree from the table. Allow multiple versions of the same non-terminal (ambiguity).
		* since the grammar is in CNF, it is a different tree (not the parse tree we are looking for), but it can be automatically transformed into the tree we are looking for
		* the resulting tree is ambiguous (we can actually read out more than one tree from the table) --> maybe model it with probabilities?
		Earley: another algorithm for parsing, a top-down one
	

11 	11/05/2015 	Statistical and Dependency Parsing (MC)	-- DEPENDENCY PARSING NOT IN THE EXAM

	Probabilistic CFGs (PCFG)
	- CFGs augmented with probabilities on rules
	- can be used to resolve ambiguity because they assign a probability to each parse tree
		probability of a parse T of a sentence S is the product of the probabilities of all n rules used to expand each of the n non-terminal nodes in the parse T
		* P(T,S) = prod_i(P(RHSi|LHSi)), where the i-th rule is given by LHSi -> RHSi
		tree with max probability
		* T*(S) = argmax_T(P(T|S)) = argmax_T(P(T,S)/P(S)) = argmax_T(P(T,S)) = argmax_T(P(T)*P(S|T)) = argmax_T(P(T)*1) = argmax_T(P(T))
	- can be used to produce a language model since it assignes a probability to the words constituting a sentence
		probability of a sentence is the sum of the probabilities of all parse trees for the sentence
		* P(S) = sum_{T s.t. S=yield(T)}(P(T,S)) = sum_{T s.t. S=yield(T)}(P(T))
		probability of the next word w_i given all the words we have seen so far (apply definition of conditional probability)
		* can be approximated using bigrams (condition only on the last word)
	Probabilistic CKY
	- add another dimension to the table. In the cell [i,j,A] store the probability of an A spanning [i,j]
	- like CFG, they have terminals, non-terminals, production rules and a starting symbol. But they have probabilities on the rules. For example, in A -> beta[p], p = P(beta|A) (and must sum up to one)
	- if we are designing a PCFG, how do we choose the probabilities? One possibility is to use counts from treebanks (corpora annotated with trees). For the unsupervised case estimate expected counts.
	- Limitations: independence from the context assumptions (the estimated probabilities for different trees can be equal, although the true ones are very different), lack of conditioning on lexical information, by PP attachment a PCFG will always prefer one attachment type
	Lexicalized PCFGs
	- PCFGs augmented to allow lexicalized rules
	- Charniak, Collins
	- Non-terminals annotated with lexical head (and the non-terminal of the head)
	- Problems: sparsity (rules are so specific that they are not observed in the corpus)
		Collins Model 1: assume independence, generate the head first, then dependents inside-out (it's just a product of all the probabilities of the head, of what is left and what is right)
	Evaluation
	- compare to human gold standard trees
